#!/usr/bin/env python3
"""
MySQL to JSON Sync Script - Bidirectional Bridge for Local Development

This script syncs data from MySQL database back to contribution JSON files.
It dynamically detects schema changes and preserves all columns from the database.

Usage:
    python3 bin/sync_mysql_to_json.py

Requirements:
    pip install mysql-connector-python
"""

import json
import os
import sys
from typing import List, Dict, Any, Set
import mysql.connector
from collections import OrderedDict


class MySQLToJSONSync:
    """Sync MySQL database to JSON contribution files with dynamic schema detection"""

    def __init__(self, host='localhost', user='root', password='root', database='world'):
        """Initialize database connection"""
        try:
            self.conn = mysql.connector.connect(
                host=host,
                user=user,
                password=password,
                database=database,
                charset='utf8mb4',
                use_unicode=True
            )
            self.cursor = self.conn.cursor(dictionary=True)
            print(f"‚úì Connected to MySQL database '{database}'")
        except mysql.connector.Error as e:
            print(f"‚ùå MySQL connection failed: {e}")
            sys.exit(1)

    def get_table_columns(self, table_name: str) -> List[str]:
        """Dynamically detect all columns in a table"""
        self.cursor.execute(f"SHOW COLUMNS FROM {table_name}")
        columns = [row['Field'] for row in self.cursor.fetchall()]
        print(f"  üìã Detected {len(columns)} columns in '{table_name}': {', '.join(columns)}")
        return columns

    def get_excluded_columns(self) -> Set[str]:
        """Columns to exclude from JSON export (internal database fields)"""
        return {'created_at', 'updated_at', 'flag'}

    def process_row(self, row: Dict[str, Any], columns: List[str], excluded: Set[str]) -> OrderedDict:
        """Process a database row into JSON-friendly format"""
        result = OrderedDict()

        for col in columns:
            if col in excluded:
                continue

            value = row.get(col)

            # Handle NULL values
            if value is None:
                result[col] = None
                continue

            # Handle JSON text fields (timezones, translations)
            if col in ['timezones', 'translations'] and isinstance(value, str):
                try:
                    result[col] = json.loads(value)
                except json.JSONDecodeError:
                    result[col] = value
            # Convert Decimal to string for coordinates
            elif col in ['latitude', 'longitude']:
                result[col] = str(value) if value is not None else None
            # Convert integer fields
            elif col == 'id' or col.endswith('_id') or col in ['level', 'population', 'gdp']:
                result[col] = int(value) if value is not None else None
            else:
                result[col] = value

        return result

    def sync_countries(self):
        """Sync countries table to contributions/countries/countries.json"""
        print("\nüì¶ Syncing countries...")

        columns = self.get_table_columns('countries')
        excluded = self.get_excluded_columns()

        self.cursor.execute("SELECT * FROM countries ORDER BY id")
        rows = self.cursor.fetchall()

        countries = []
        for row in rows:
            countries.append(self.process_row(row, columns, excluded))

        output_file = os.path.join('contributions', 'countries', 'countries.json')
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(countries, f, ensure_ascii=False, indent=2)

        print(f"  ‚úì Synced {len(countries)} countries to {output_file}")
        return len(countries)

    def sync_states(self):
        """Sync states table to contributions/states/states.json"""
        print("\nüì¶ Syncing states...")

        columns = self.get_table_columns('states')
        excluded = self.get_excluded_columns()

        self.cursor.execute("SELECT * FROM states ORDER BY id")
        rows = self.cursor.fetchall()

        states = []
        for row in rows:
            states.append(self.process_row(row, columns, excluded))

        output_file = os.path.join('contributions', 'states', 'states.json')
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(states, f, ensure_ascii=False, indent=2)

        print(f"  ‚úì Synced {len(states)} states to {output_file}")
        return len(states)

    def sync_cities(self):
        """Sync cities table to contributions/cities/<COUNTRY_CODE>.json files"""
        print("\nüì¶ Syncing cities...")

        columns = self.get_table_columns('cities')
        excluded = self.get_excluded_columns()

        # Get all unique country codes
        self.cursor.execute("SELECT DISTINCT country_code FROM cities ORDER BY country_code")
        country_codes = [row['country_code'] for row in self.cursor.fetchall()]

        total_cities = 0
        cities_dir = os.path.join('contributions', 'cities')

        # Ensure cities directory exists
        os.makedirs(cities_dir, exist_ok=True)

        for country_code in country_codes:
            # Fetch all cities for this country
            query = f"SELECT * FROM cities WHERE country_code = %s ORDER BY id"
            self.cursor.execute(query, (country_code,))
            rows = self.cursor.fetchall()

            cities = []
            for row in rows:
                cities.append(self.process_row(row, columns, excluded))

            # Save to country-specific JSON file
            output_file = os.path.join(cities_dir, f'{country_code}.json')
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(cities, f, ensure_ascii=False, indent=2)

            print(f"  ‚úì {country_code}: {len(cities):,} cities ‚Üí {output_file}")
            total_cities += len(cities)

        print(f"\n  ‚úì Total: {total_cities:,} cities synced to {len(country_codes)} files")
        return total_cities

    def sync_regions(self):
        """Sync regions table (if you need it in contributions)"""
        print("\nüì¶ Syncing regions...")

        columns = self.get_table_columns('regions')
        excluded = self.get_excluded_columns()

        self.cursor.execute("SELECT * FROM regions ORDER BY id")
        rows = self.cursor.fetchall()

        regions = []
        for row in rows:
            regions.append(self.process_row(row, columns, excluded))

        # Create regions directory if it doesn't exist
        regions_dir = os.path.join('contributions', 'regions')
        os.makedirs(regions_dir, exist_ok=True)

        output_file = os.path.join(regions_dir, 'regions.json')
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(regions, f, ensure_ascii=False, indent=2)

        print(f"  ‚úì Synced {len(regions)} regions to {output_file}")
        return len(regions)

    def sync_subregions(self):
        """Sync subregions table (if you need it in contributions)"""
        print("\nüì¶ Syncing subregions...")

        columns = self.get_table_columns('subregions')
        excluded = self.get_excluded_columns()

        self.cursor.execute("SELECT * FROM subregions ORDER BY id")
        rows = self.cursor.fetchall()

        subregions = []
        for row in rows:
            subregions.append(self.process_row(row, columns, excluded))

        # Create subregions directory if it doesn't exist
        subregions_dir = os.path.join('contributions', 'subregions')
        os.makedirs(subregions_dir, exist_ok=True)

        output_file = os.path.join(subregions_dir, 'subregions.json')
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(subregions, f, ensure_ascii=False, indent=2)

        print(f"  ‚úì Synced {len(subregions)} subregions to {output_file}")
        return len(subregions)

    def close(self):
        """Close database connection"""
        self.cursor.close()
        self.conn.close()


def main():
    """Main execution"""
    print("üîÑ MySQL ‚Üí JSON Sync (Dynamic Schema Detection)\n")
    print("=" * 60)

    # Change to project root directory
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # Go up 3 levels: sync/ -> scripts/ -> bin/ -> project_root
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(script_dir)))
    os.chdir(project_root)

    # Initialize sync
    syncer = MySQLToJSONSync(
        host='localhost',
        user='root',
        password='',
        database='world'
    )

    try:
        # Sync all tables
        countries_count = syncer.sync_countries()
        states_count = syncer.sync_states()
        cities_count = syncer.sync_cities()

        # Optional: Sync regions and subregions if needed
        # regions_count = syncer.sync_regions()
        # subregions_count = syncer.sync_subregions()

        print("\n" + "=" * 60)
        print("‚úÖ Sync complete!")
        print(f"   üìç Countries: {countries_count}")
        print(f"   üìç States: {states_count}")
        print(f"   üìç Cities: {cities_count:,}")
        print("\nüí° Next steps:")
        print("   1. Review changes: git diff")
        print("   2. Build JSON: python3 bin/build_from_contributions.py")
        print("   3. Commit: git add . && git commit -m 'sync: update from MySQL'")

    except Exception as e:
        print(f"\n‚ùå Sync failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        syncer.close()


if __name__ == '__main__':
    main()
